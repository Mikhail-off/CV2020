{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c86YVEB-blig"
   },
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks (GANs) are an approach to generative modeling based on deep learning methods.\n",
    "\n",
    "The standard problem settings for GANs are generation of photorealistic images and image-to-image translation tasks (translating photos of summer to winter, day to night etc.).\n",
    "\n",
    "In this task you will familiarize yourself with both these problems while trying to create fake images of sneakers. \n",
    "\n",
    "As GANs still do have certain limitations about generating large images, the task is decomposed into two: first, use a simple GAN to generate a bunch of low resolution images from noise, then upscale them using another generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0KO1uFkrmZF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "v071WBN4rmZM",
    "outputId": "8be45131-37f6-490a-d890-3baa48a6912e"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "L = keras.layers\n",
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Mk1t4_jqIzCv",
    "outputId": "0d7eedc3-b0ca-4f8c-8943-ca646e7c3a60"
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/7dnwfvqxstr3j4v/data.zip?dl=0\" -O data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xtiNA0YtJDha",
    "outputId": "d308b2f7-56eb-42de-ee47-c1e5a62dfd32"
   },
   "outputs": [],
   "source": [
    "dataset_root = \"./data\"\n",
    "images_dir = \"images\"\n",
    "image_filenames = sorted(os.listdir(os.path.join(dataset_root, images_dir)))\n",
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgjLoSrNvm5l"
   },
   "source": [
    "# I. Image Generation (5 pts)\n",
    "\n",
    "Your first task is to solve a problem of generating photorealistic images out of noise (okay, this might sound optimistic).\n",
    "\n",
    "Namely, you are required to **create fake images of sneakers of resolution 28x28 given a vector of noise sampled from standard normal distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvrKevxRIH0O"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0k2VIfjptkt"
   },
   "outputs": [],
   "source": [
    "def img2data(image):\n",
    "    data = 2 * image - 1\n",
    "    return data\n",
    "\n",
    "def data2img(data):\n",
    "    image = 0.5 * data + 0.5\n",
    "    return image\n",
    "\n",
    "def load_image(filename, target_size=None):\n",
    "    image_bgr = cv2.imread(os.path.join(dataset_root, images_dir, filename))\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if target_size is not None:\n",
    "        image_rgb = cv2.resize(image_rgb, target_size)\n",
    "\n",
    "    image_rgb = (image_rgb / 255).astype(np.float32)\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ch0DUEiCLtwd",
    "outputId": "b21e96ad-d2d0-42f7-a49e-772ba26f59ed"
   },
   "outputs": [],
   "source": [
    "all_data = np.stack([\n",
    "    img2data(load_image(fn, IMAGE_SIZE)) \n",
    "    for fn in tqdm.tqdm(image_filenames, position=0)\n",
    "])\n",
    "assert all_data.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "giU-X5NTMA3s"
   },
   "outputs": [],
   "source": [
    "def sample_data(batch_size):\n",
    "    batch_ids = np.random.choice(len(all_data), size=batch_size)\n",
    "    return all_data[batch_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8R4aJ0z3My9g"
   },
   "outputs": [],
   "source": [
    "def visualize_images(data, n_rows, n_cols):\n",
    "    n_samples = n_rows * n_cols\n",
    "\n",
    "    if len(data) != n_samples:\n",
    "        sample_indices = np.random.choice(len(data), n_samples, replace=len(data) < n_samples)\n",
    "    else:\n",
    "        sample_indices = np.arange(len(data)).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(int(2.5 * n_cols), int(2.5 * n_rows)))\n",
    "    for i, sample_index in enumerate(sample_indices):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(data2img(data[sample_index]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# draw random images\n",
    "visualize_images(sample_data(32), 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG7v44CaIH0e"
   },
   "source": [
    "## Random Noise\n",
    "Generate a TensorFlow `Tensor` containing gaussian noise of shape `[batch_size, dim]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tclu9GacIH0g"
   },
   "outputs": [],
   "source": [
    "NOISE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVriOM06rmZO"
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size):\n",
    "    return np.random.randn(batch_size, NOISE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RN1YbjRJXQGo"
   },
   "source": [
    "## Fully connected GAN (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCqR7VXaXz_U"
   },
   "source": [
    "### Generator (0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xhSPMmhXU-7"
   },
   "source": [
    "Our first step is to build a generator. You should use the layers in `tf.keras.layers` to construct the model. All fully connected layers should include bias terms. Use the default initializers for parameters.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 28 x 28 x 3\n",
    " * Reshape into (28, 28, 3)\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0q73nGoFo-Sn"
   },
   "outputs": [],
   "source": [
    "def build_fc_generator():\n",
    "    inputs = L.Input(shape=(NOISE_SIZE,))\n",
    "\n",
    "    # TODO: build the model\n",
    "    \n",
    "    outputs = \"???\"\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=\"FCGAN_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knZ3OQzcSH2z"
   },
   "outputs": [],
   "source": [
    "# Build FCGAN generator\n",
    "generator = build_fc_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ301b-UBI-y"
   },
   "outputs": [],
   "source": [
    "noise = sample_noise(32)\n",
    "fake_data = generator.predict(noise)\n",
    "assert fake_data.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9kJgtu5X5nT"
   },
   "source": [
    "### Discriminator (0.5 pt)\n",
    "\n",
    "Now you are to build a discriminator. Same as for generator, all fully connected layers should include bias terms. You should use the default initializers for parameters here as well.\n",
    "\n",
    "Architecture:\n",
    " * Flatten\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 1\n",
    " * Sigmoid (to obtain logits as an output)\n",
    "\n",
    "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the probability that each of the `batch_size` inputs is a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qq4X_E9Eorvz"
   },
   "outputs": [],
   "source": [
    "def build_fc_discriminator():\n",
    "    inputs = L.Input(shape=IMAGE_SIZE + (3,))\n",
    "    \n",
    "    # TODO: build the model\n",
    "    \n",
    "    outputs = \"???\"\n",
    "\n",
    "    return keras.models.Model(inputs, outputs, name=\"FCGAN_disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HEMfyj8rmZX"
   },
   "outputs": [],
   "source": [
    "# Build FCGAN discriminator\n",
    "discriminator = build_fc_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_OkluLrBsqt"
   },
   "outputs": [],
   "source": [
    "fake_probas = discriminator.predict(fake_data)\n",
    "assert fake_probas.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKL-StffIH0_"
   },
   "source": [
    "## GAN Loss (1 pt)\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "Instead of computing the expectation, you may average over elements of the minibatch, so make sure to combine the loss by *averaging* instead of summing.\n",
    "\n",
    "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip2eDhj3rmZf"
   },
   "outputs": [],
   "source": [
    "def generator_gan_loss(y_true, y_pred):\n",
    "    fake_probas = \"???\"\n",
    "    logp_fake_is_real = \"???\"\n",
    "    loss = \"???\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDn4-gxHCdOs"
   },
   "outputs": [],
   "source": [
    "loss = generator_gan_loss(None, fake_probas)\n",
    "assert loss.shape[1:] == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1ttqSiXrmZj"
   },
   "outputs": [],
   "source": [
    "def discriminator_gan_loss(y_true, y_pred, eps=K.epsilon()):\n",
    "    probas = K.clip(y_pred, eps, 1 - eps) # clip to avoid overflow\n",
    "\n",
    "    is_real = y_true\n",
    "    is_fake = 1 - y_true\n",
    "\n",
    "    logp_real_is_real = is_real * \"???\"\n",
    "    logp_fake_is_fake = is_fake * \"???\"\n",
    "\n",
    "    loss = \"???\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NK1XsxYAEGak"
   },
   "outputs": [],
   "source": [
    "real_data = sample_data(32)\n",
    "\n",
    "real_and_fake_data = np.concatenate((real_data, fake_data))\n",
    "real_and_fake_labels = [1] * len(real_data) + [0] * len(fake_data)\n",
    "\n",
    "real_and_fake_probas = discriminator.predict(real_and_fake_data)\n",
    "\n",
    "loss = discriminator_gan_loss(K.variable(real_and_fake_labels), K.variable(real_and_fake_probas))\n",
    "assert loss.shape[1:] == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9VhgWjecJ4u"
   },
   "outputs": [],
   "source": [
    "discriminator_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_loss = \"???\"\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=discriminator_opt,\n",
    "    loss=discriminator_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA_7JQon3Ud2"
   },
   "outputs": [],
   "source": [
    "# we only want to be able to train generation for the combined model\n",
    "discriminator.trainable = False\n",
    "\n",
    "combined = keras.models.Model(\n",
    "    inputs=generator.inputs,\n",
    "    outputs=\"???\",\n",
    "    name=\"FCGAN_combined\"\n",
    ")\n",
    "\n",
    "generator_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
    "generator_loss = \"???\"\n",
    "\n",
    "combined.compile(\n",
    "    optimizer=generator_opt,\n",
    "    loss=generator_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_bwW1wbrmZo"
   },
   "source": [
    "**Utility functions**\n",
    "\n",
    "Just a few helper functions that draw current data distributions and sample training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uQxaSmprmZp"
   },
   "outputs": [],
   "source": [
    "def sample_images(n_rows, n_cols):\n",
    "    batch_size = n_rows * n_cols\n",
    "    data = generator.predict(sample_noise(batch_size))\n",
    "    visualize_images(data, n_rows, n_cols)\n",
    "\n",
    "\n",
    "def sample_probas(num_samples, batch_size=1):\n",
    "    plt.title(\"Fake vs real distribution\")\n",
    "    \n",
    "    real_probas, fake_probas = [], []\n",
    "    \n",
    "    for _ in range(0, num_samples, batch_size):\n",
    "        real_data = sample_data(batch_size)\n",
    "        fake_data = generator.predict(sample_noise(batch_size))\n",
    "        \n",
    "        real_probas.append(discriminator.predict(real_data))\n",
    "        fake_probas.append(discriminator.predict(fake_data))\n",
    "\n",
    "    real_probas = np.concatenate(real_probas, 0)[:num_samples]\n",
    "    fake_probas = np.concatenate(fake_probas, 0)[:num_samples]\n",
    "\n",
    "    plt.hist(real_probas, label=\"D(x)\", color='g', alpha=0.5, range=[0,1])\n",
    "    plt.hist(fake_probas, label=\"D(G(z))\", color='r', alpha=0.5, range=[0,1])\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-ZN6AjRrmZt"
   },
   "source": [
    "### Training\n",
    "Train generator and discriminator in a loop and draw results once every N iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrwnzEOsrmZv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5000\n",
    "\n",
    "history = {\"gen\": [], \"disc\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate a new batch of noise\n",
    "    noise = \"???\"\n",
    "\n",
    "    # Get a batch of real images\n",
    "    real_data = \"???\"\n",
    "\n",
    "    # Generate a batch of fake images\n",
    "    # Hint: use .predict() method\n",
    "    fake_data = \"???\"\n",
    "\n",
    "    # Run discriminator on both real and fake images\n",
    "    # Hint: use .train_on_batch() method\n",
    "    loss_d_real = \"???\" # train on real_data\n",
    "    loss_d_fake = \"???\" # train on fake_data\n",
    "    loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
    "\n",
    "    # Make another noise batch\n",
    "    noise = \"???\"\n",
    "\n",
    "    # Train the generator to trick the discriminator\n",
    "    # For the generator, we want all the {fake, not-fake} labels \n",
    "    # to say not-fake\n",
    "    # Hint: use combined model\n",
    "    loss_g = \"???\"\n",
    "\n",
    "    history[\"disc\"].append(loss_d)\n",
    "    history[\"gen\"].append(loss_g)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch:\", epoch)\n",
    "        \n",
    "        sample_images(2, 4)\n",
    "        sample_probas(100, batch_size)\n",
    "\n",
    "        time = np.arange(epoch + 1)\n",
    "\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(13, 4))\n",
    "        ax[0].set_title(\"Generator loss\")\n",
    "        ax[0].plot(time, history[\"gen\"], color='g')\n",
    "        ax[1].set_title(\"Discriminator loss\")\n",
    "        ax[1].plot(time, history[\"disc\"], color='r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_sdUY7mIH1k"
   },
   "source": [
    "## Deep Convolutional GANs (2 pts)\n",
    "In the first part of the notebook, you have implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, you are to implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where both discriminator and generator are convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8oypyovIH0n"
   },
   "source": [
    "### Generator (1 pt)\n",
    "\n",
    "Architecture:\n",
    " * Fully connected with output size of 128x7x7\n",
    " * Reshape into (7, 7, 128)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=128, padding=\"same\"\n",
    " * Batch Normalization with momentum(0.8)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=64, padding=\"same\"\n",
    " * Batch Normalization with momentum(0.8)\n",
    " * ReLU\n",
    " * Conv2D: 3x3, filters=3, padding=\"same\"\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpEnIgOsoJnV"
   },
   "outputs": [],
   "source": [
    "def build_dc_generator():\n",
    "    inputs = L.Input(shape=(NOISE_SIZE,))\n",
    "\n",
    "    #TODO: build the model\n",
    "    \n",
    "    outputs = \"???\"\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=\"DCGAN_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exTI31jBUZzq"
   },
   "outputs": [],
   "source": [
    "# Build DCGAN generator\n",
    "generator = build_dc_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrbwTOJBDpNM"
   },
   "outputs": [],
   "source": [
    "noise = sample_noise(32)\n",
    "fake_data = generator.predict(noise)\n",
    "assert fake_data.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4lr8-SGrmZU"
   },
   "source": [
    "### Discriminator (1  pt)\n",
    "\n",
    "Architecture:\n",
    " * Conv2D: 3x3, filters=32, strides=2, padding=\"same\"\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=64, strides=2, padding=\"same\"\n",
    " * Zero Padding: ((0, 1), (0, 1))\n",
    " * Batch Normalization with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=128, strides=2, padding=\"same\"\n",
    " * Batch Normalization with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=256, strides=2, padding=\"same\"\n",
    " * Batch Normalization with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Flatten\n",
    " * Fully connected layer with output size 1\n",
    " * Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEaB4n7NUD2Z"
   },
   "outputs": [],
   "source": [
    "def build_dc_discriminator():\n",
    "    inputs = L.Input(shape=(IMAGE_SIZE + (3,)))\n",
    "\n",
    "    #TODO: build the model\n",
    "    \n",
    "    outputs = \"???\"\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=\"DCGAN_disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uP-vPXqpoEKI"
   },
   "outputs": [],
   "source": [
    "# Build DCGAN discriminator\n",
    "discriminator = build_dc_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1x7nQr-Fey5"
   },
   "outputs": [],
   "source": [
    "fake_probas = discriminator.predict(fake_data)\n",
    "assert fake_probas.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVJFygbNIH1g"
   },
   "source": [
    "## Least Squares GAN loss (1 pt)\n",
    "We'll now look at [Least Squares GAN loss](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all you have to do is change the loss function and retrain the model. You'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "and the discriminator loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TN9FlxXWr1v"
   },
   "outputs": [],
   "source": [
    "def generator_ls_loss(y_true, y_pred):\n",
    "    # Implement Least Squares GAN loss for generator\n",
    "    loss = \"???\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2s7zIcdDaBF"
   },
   "outputs": [],
   "source": [
    "loss = generator_ls_loss(None, K.variable(fake_probas))\n",
    "assert loss.shape[1:] == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctY6vasxIH1h"
   },
   "outputs": [],
   "source": [
    "def discriminator_ls_loss(y_true, y_pred):\n",
    "    # Implement Least Squares GAN loss for discriminator\n",
    "    is_real = y_true\n",
    "    is_fake = 1 - y_true\n",
    "\n",
    "    sq_err_real_and_1 = is_real * \"???\"\n",
    "    sq_err_fake_and_0 = is_fake * \"???\"\n",
    "\n",
    "    loss = \"???\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_y_trcJIE1HV"
   },
   "outputs": [],
   "source": [
    "real_data = sample_data(32)\n",
    "\n",
    "real_and_fake_data = np.concatenate((real_data, fake_data))\n",
    "real_and_fake_labels = np.array([1] * len(real_data) + [0] * len(fake_data))\n",
    "\n",
    "real_and_fake_probas = discriminator.predict(real_and_fake_data)\n",
    "\n",
    "loss = discriminator_ls_loss(K.variable(real_and_fake_labels), K.variable(real_and_fake_probas))\n",
    "assert loss.shape[1:] == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrKufXVGOu__"
   },
   "source": [
    "Last step is to complile models with proper optimizers and losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8h30_ig_OGX"
   },
   "outputs": [],
   "source": [
    "discriminator_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_loss = \"???\"\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=discriminator_opt,\n",
    "    loss=discriminator_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bap1-mAK_XAR"
   },
   "outputs": [],
   "source": [
    "# we only want to be able to train generation for the combined model\n",
    "discriminator.trainable = False\n",
    "\n",
    "combined = keras.models.Model(\n",
    "    inputs=generator.inputs,\n",
    "    outputs=\"???\",\n",
    "    name=\"DCGAN_combined\"\n",
    ")\n",
    "\n",
    "generator_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
    "generator_loss = \"???\"\n",
    "\n",
    "combined.compile(\n",
    "    optimizer=generator_opt,\n",
    "    loss=generator_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn4Fc7n4e98i"
   },
   "source": [
    "### Training\n",
    "\n",
    "Here, training loop is identical to the one for FCGAN in the previous task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3KzaV7GS4FD"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5000\n",
    "\n",
    "history = {\"gen\": [], \"disc\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate a new batch of noise\n",
    "    noise = \"???\"\n",
    "\n",
    "    # Get a batch of real images\n",
    "    real_data = \"???\"\n",
    "\n",
    "    # Generate a batch of fake images\n",
    "    # Hint: use .predict() method\n",
    "    fake_data = \"???\"\n",
    "\n",
    "    # Run discriminator on both real and fake images\n",
    "    # Hint: use .train_on_batch() method\n",
    "    loss_d_real = \"???\" # train on real_data\n",
    "    loss_d_fake = \"???\" # train on fake_data\n",
    "    loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
    "\n",
    "    # Make another noise batch\n",
    "    noise = \"???\"\n",
    "\n",
    "    # Train the generator to trick the discriminator\n",
    "    # For the generator, we want all the {fake, not-fake} labels \n",
    "    # to say not-fake\n",
    "    # Hint: use combined model\n",
    "    loss_g = \"???\"\n",
    "\n",
    "    history[\"disc\"].append(loss_d)\n",
    "    history[\"gen\"].append(loss_g)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch:\", epoch)\n",
    "        \n",
    "        sample_images(2, 4)\n",
    "        sample_probas(100, batch_size)\n",
    "\n",
    "        time = np.arange(epoch + 1)\n",
    "\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(13, 4))\n",
    "        ax[0].set_title(\"Generator loss\")\n",
    "        ax[0].plot(time, history[\"gen\"], color='g')\n",
    "        ax[1].set_title(\"Discriminator loss\")\n",
    "        ax[1].plot(time, history[\"disc\"], color='r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nevOOmt2-VRR"
   },
   "source": [
    "Save some generated images for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjMBKI2v-a17"
   },
   "outputs": [],
   "source": [
    "all_fake_data = []\n",
    "for _ in range(0, len(all_data), batch_size):\n",
    "    fake_data = generator.predict(sample_noise(batch_size))\n",
    "    all_fake_data.append(fake_data)\n",
    "\n",
    "all_fake_data = np.concatenate(all_fake_data, 0)[:len(all_data)]\n",
    "assert all_fake_data.shape == all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKxYVoOzNrte"
   },
   "outputs": [],
   "source": [
    "visualize_images(all_fake_data, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"fake_data.npy\", all_fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99EeW8dYvddK"
   },
   "source": [
    "# II. Super Resolution (3 pts) \n",
    "\n",
    "In this part of the notebook you will train a generative model that solves an image-to-image problem, with \"small images\" as a source domain and \"large images\" being a target domain. \n",
    "\n",
    "To specify the task, you are to **scale small images of 28x28 pixels up to size of 112x112 pixels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fh6j4KTTXeeZ"
   },
   "outputs": [],
   "source": [
    "LOW_RES_SIZE = IMAGE_SIZE\n",
    "HIGH_RES_SIZE = (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hScGuZ4yNL4G"
   },
   "outputs": [],
   "source": [
    "all_data_hr = np.stack([\n",
    "    img2data(load_image(fn, HIGH_RES_SIZE))\n",
    "    for fn in tqdm.tqdm(image_filenames, position=0)\n",
    "])\n",
    "assert all_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfHzIL34fvWx"
   },
   "outputs": [],
   "source": [
    "def sample_super_res_data(batch_size, low_res_size=LOW_RES_SIZE, high_res_size=HIGH_RES_SIZE):\n",
    "    batch_ids = np.random.choice(len(all_data), size=batch_size)\n",
    "    return all_data[batch_ids], all_data_hr[batch_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5T6vEh6LAuP"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "real_data_lr, real_data_hr = sample_super_res_data(batch_size)\n",
    "\n",
    "assert real_data_lr.shape[1:] == LOW_RES_SIZE + (3,)\n",
    "assert real_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)\n",
    "\n",
    "plt.figure(figsize=(8, 14))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(batch_size, 2, 2 * i + 1)\n",
    "    plt.title(\"Low resolution\")\n",
    "    plt.imshow(data2img(real_data_lr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(batch_size, 2, 2 * i + 2)\n",
    "    plt.title(\"High resolution\")\n",
    "    plt.imshow(data2img(real_data_hr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6tW803scFAG"
   },
   "source": [
    "In the second part of this task, you are to train an [SRGAN](https://arxiv.org/abs/1609.04802)-like model. For your convinience, some layers are already implemented. Now it's your turn -- fill the gaps so the model passes the asserts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qj3smKR47wGg"
   },
   "source": [
    "### Generator (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaEvFycwBuRP"
   },
   "source": [
    "To build a SRGAN Generator, you will need a basic Residual Block(filters):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU\n",
    "* Batch Normalization with momentum(0.8)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* Batch Normalization with momentum(0.8)\n",
    "* Sum up outputs with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdcGWPx5IH1l"
   },
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, **kwargs):\n",
    "    \n",
    "    # TODO: build the block\n",
    "    \n",
    "    outputs = \"???\"\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-ddz1idCkar"
   },
   "source": [
    "... and an Upsampling Block(filters):\n",
    "\n",
    "* UpSampling2D(2)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fE-6M0iJClX9"
   },
   "outputs": [],
   "source": [
    "def up_block(inputs, filters, **kwargs):\n",
    "    \n",
    "    # TODO: build the block\n",
    "    \n",
    "    outputs = \"???\"\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b__NKRfbL7CY"
   },
   "source": [
    "Now, using these basic building blocks, one is able to define a SRGAN generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fu1R1eDf1iMd"
   },
   "outputs": [],
   "source": [
    "def build_sr_generator():\n",
    "    inputs = L.Input(shape=LOW_RES_SIZE + (3,))\n",
    "\n",
    "    # An initial conv block\n",
    "    x = L.Conv2D(64, kernel_size=9, strides=1, padding=\"same\")(inputs)\n",
    "    x = L.ReLU()(x)\n",
    "\n",
    "    conv = x\n",
    "\n",
    "    # 16 residual blocks with filters=64\n",
    "    for block_num in range(16):\n",
    "        x = \"???\"\n",
    "\n",
    "    # A post-residual block\n",
    "    x = L.Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = L.BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    # Sum up current 'x' with 'conv' obtained earlier\n",
    "    x = \"???\"\n",
    "\n",
    "    # Two upsampling blocks with filters=256\n",
    "    x = \"???\"\n",
    "\n",
    "    # Final conv block\n",
    "    x = L.Conv2D(3, kernel_size=9, strides=1, padding=\"same\")(x)\n",
    "    outputs = L.Activation(\"tanh\")(x)\n",
    "    \n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=\"SRGAN_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB8Yo2mIHmJ_"
   },
   "outputs": [],
   "source": [
    "# Build SRGAN generator\n",
    "generator = build_sr_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4gZAYisFthk"
   },
   "outputs": [],
   "source": [
    "real_data_lr, real_data_hr = sample_super_res_data(32)\n",
    "fake_data_hr = generator.predict(real_data_lr)\n",
    "assert fake_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn19KmZGcI7W"
   },
   "source": [
    "### Discriminator (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRnegUvu9EOL"
   },
   "source": [
    "First, define a Discriminator Block(filters, strides):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=strides, padding=\"same\"\n",
    "* Leaky ReLU(0.2)\n",
    "* (optional) Batch Normalization with momentum(0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6r3nv0Q9Cwm"
   },
   "outputs": [],
   "source": [
    "def d_block(inputs, filters, strides=1, apply_batchnorm=True, **kwargs):\n",
    "    \n",
    "    # TODO: build the block\n",
    "    \n",
    "    outputs = \"???\"\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okGOSTJfAXxr"
   },
   "source": [
    "Now, use this block to build up SRGAN discriminator:\n",
    "\n",
    "* DBlock(filters=64, strides=1) with Batch Normalization\n",
    "* DBlock(filters=64, strides=2)\n",
    "* DBlock(filters=128, strides=1)\n",
    "* DBlock(filters=128, strides=2)\n",
    "* DBlock(filters=256, strides=1)\n",
    "* DBlock(filters=256, strides=2)\n",
    "* DBlock(filters=512, strides=1)\n",
    "* DBlock(filters=512, strides=2)\n",
    "* Flatten\n",
    "* Fully connected with output size of 1024\n",
    "* Leaky ReLU(0.2)\n",
    "* Fully connected with output size of 1\n",
    "* Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcA-3n5mbhdk"
   },
   "outputs": [],
   "source": [
    "def build_sr_discriminator():\n",
    "    inputs = L.Input(shape=HIGH_RES_SIZE + (3,))\n",
    "\n",
    "    # TODO: build the model\n",
    "    \n",
    "    outputs = \"???\"\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs, name=\"SRGAN_disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lT8pojPBHwoT"
   },
   "outputs": [],
   "source": [
    "# Build SRGAN discriminator\n",
    "discriminator = build_sr_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgkRNXNFFlgT"
   },
   "outputs": [],
   "source": [
    "fake_probas = discriminator.predict(fake_data_hr)\n",
    "assert fake_probas.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zz5H-X95FEAW"
   },
   "outputs": [],
   "source": [
    "real_data_lr, real_data_hr = sample_super_res_data(32)\n",
    "real_probas = discriminator.predict(real_data_hr)\n",
    "\n",
    "real_data = sample_data(32)\n",
    "\n",
    "real_and_fake_data_hr = np.concatenate((real_data_hr, fake_data_hr))\n",
    "real_and_fake_labels = np.array([1] * len(real_data_hr) + [0] * len(fake_data_hr))\n",
    "\n",
    "real_and_fake_probas = discriminator.predict(real_and_fake_data_hr)\n",
    "\n",
    "loss = discriminator_ls_loss(K.variable(real_and_fake_labels), K.variable(real_and_fake_probas))\n",
    "assert loss.shape[1:] == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sItac4faDHdi"
   },
   "source": [
    "Typically, SRGAN is trained with additional loss on features from pretrained VGG-19. However, you task will be a bit simplier: use **mean squared error** between real and fake data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3rubdLALfHyB"
   },
   "outputs": [],
   "source": [
    "def feature_loss(real_features, fake_features):\n",
    "    # Implement MSE\n",
    "    loss = \"???\"\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXeSQekeCLAj"
   },
   "outputs": [],
   "source": [
    "loss = feature_loss(K.variable(real_data_hr), K.variable(fake_data_hr))\n",
    "assert loss.shape == ()\n",
    "K.eval(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enlXr3tBBQJ2"
   },
   "outputs": [],
   "source": [
    "discriminator_opt = keras.optimizers.Adam(1e-3, 0.5)\n",
    "discriminator_loss = \"???\" # you may choose any or try both. \n",
    "#By default, opt for the Least Squares GAN loss.\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer=discriminator_opt,\n",
    "    loss=discriminator_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbNUPXJVBI_Q"
   },
   "outputs": [],
   "source": [
    "# we only want to be able to train generation for the combined model\n",
    "discriminator.trainable = False\n",
    "\n",
    "outputs = generator.outputs\n",
    "\n",
    "combined = keras.models.Model(\n",
    "    inputs=generator.inputs,\n",
    "    outputs=\"???\", # you should return both probas AND generated images\n",
    "    name=\"SRGAN_combined\"\n",
    ")\n",
    "\n",
    "generator_opt = keras.optimizers.Adam(2e-4, 0.5)\n",
    "generator_loss = \"???\" # mind the order of outputs -- it should match the losses\n",
    "loss_weights = [1, 0.5]\n",
    "\n",
    "combined.compile(\n",
    "    optimizer=generator_opt,\n",
    "    loss=generator_loss,\n",
    "    loss_weights=loss_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avdC_UIgJw6t"
   },
   "outputs": [],
   "source": [
    "def sample_images(n_samples):\n",
    "    batch_size = n_samples\n",
    "    real_data_lr, real_data_hr = sample_super_res_data(batch_size)\n",
    "    fake_data_hr = generator.predict(real_data_lr)\n",
    "\n",
    "    plt.figure(figsize=(11, int(3.5 * n_samples)))\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(n_samples, 3, 3 * i + 1)\n",
    "        plt.title(\"Original LR\")\n",
    "        plt.imshow(data2img(real_data_lr[i]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(n_samples, 3, 3 * i + 3)\n",
    "        plt.title(\"Generated HR\")\n",
    "        plt.imshow(data2img(fake_data_hr[i]))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.subplot(n_samples, 3, 3 * i + 2)\n",
    "        plt.title(\"Real HR\")\n",
    "        plt.imshow(data2img(real_data_hr[i]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample_probas(num_samples, batch_size):\n",
    "    plt.title(\"Fake vs real distribution\")\n",
    "    \n",
    "    real_probas, fake_probas = [], []\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        real_data_lr, real_data_hr = sample_super_res_data(batch_size)\n",
    "        fake_data_hr = generator.predict(real_data_lr)\n",
    "        \n",
    "        real_probas.append(discriminator.predict(real_data_hr))\n",
    "        fake_probas.append(discriminator.predict(fake_data_hr))\n",
    "\n",
    "    real_probas = np.concatenate(real_probas, 0)[:num_samples]\n",
    "    fake_probas = np.concatenate(fake_probas, 0)[:num_samples]\n",
    "\n",
    "    plt.hist(real_probas, label=\"D(x)\", alpha=0.5, range=[0,1], color='g')\n",
    "    plt.hist(fake_probas, label=\"D(G(z))\", alpha=0.5, range=[0,1], color='r')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdC_vNP9xlIZ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10-xECeBbwmd"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 5000\n",
    "\n",
    "history = {\"gen\": [], \"disc\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Sample low-res and high-res data\n",
    "    real_data_lr, real_data_hr = \"???\"\n",
    "\n",
    "    # Generate high resolution images from low resolution ones\n",
    "    fake_data_hr = \"???\"\n",
    "\n",
    "    # Run discriminator on both real and fake high resolution images\n",
    "    loss_d_real = \"???\" # train on real_data_hr\n",
    "    loss_d_fake = \"???\" # train on fake_data_hr\n",
    "    loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
    "\n",
    "    # Sample another batch of low-res and high-res data\n",
    "    real_data_lr, real_data_hr = \"???\"\n",
    "\n",
    "    # Train the generator to trick the discriminator\n",
    "    # For the generator, we want all the {fake, not-fake} labels \n",
    "    # to say not-fake\n",
    "    # Mind the number and the order of the outputs of the combined model!\n",
    "    loss_g = \"???\"\n",
    "\n",
    "    history[\"gen\"].append(loss_g) \n",
    "    history[\"disc\"].append(loss_d)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch:\", epoch)\n",
    "        \n",
    "        sample_images(4)\n",
    "        sample_probas(100, batch_size)\n",
    "\n",
    "        time = np.arange(epoch + 1)\n",
    "\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(13, 4))\n",
    "        ax[0].set_title(\"Generator loss\")\n",
    "        ax[0].plot(time, [_[0] for _ in history[\"gen\"]], color=\"dodgerblue\", label=\"GAN loss\")\n",
    "        ax[0].plot(time, [_[1] for _ in history[\"gen\"]], color=\"gold\", label=\"Feature loss\")\n",
    "        ax[0].plot(time, [_[2] for _ in history[\"gen\"]], color='g', label=\"Total loss\")\n",
    "        ax[0].legend(loc=\"upper left\")\n",
    "        ax[1].set_title(\"Discriminator loss\")\n",
    "        ax[1].plot(time, history[\"disc\"], color='r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSya-x9pAsmd"
   },
   "source": [
    "Save some fake high resolution images generated from real low resolution samples stored in `all_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_yVB5WNrmZz"
   },
   "outputs": [],
   "source": [
    "all_fake_data_from_real_lr = []\n",
    "for index in tqdm.trange(0, len(all_data), batch_size, position=0):\n",
    "    real_data_lr, real_data_hr = sample_super_res_data(batch_size) \n",
    "    fake_data_hr = generator.predict(real_data_lr)\n",
    "    all_fake_data_from_real_lr.append(fake_data_hr)\n",
    "\n",
    "all_fake_data_from_real_lr = np.concatenate(all_fake_data_from_real_lr, 0)[:len(all_data)]\n",
    "assert all_fake_data_from_real_lr.shape == all_data_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2mkAs09KbiI"
   },
   "outputs": [],
   "source": [
    "visualize_images(all_fake_data_from_real_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I43NtR1Ayic-"
   },
   "outputs": [],
   "source": [
    "np.save(\"fake_data_from_real_lr.npy\", all_fake_data_from_real_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGMrNbHqA4-u"
   },
   "source": [
    "Now save some fake high resolution images generated from fake low resolution samples stored in `all_fake_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_fake_data\n",
    "except:\n",
    "    all_fake_data = np.load(\"fake_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_cXEpWSApVp"
   },
   "outputs": [],
   "source": [
    "all_fake_data_from_fake_lr = []\n",
    "for index in tqdm.trange(0, len(all_data), batch_size, position=0):\n",
    "    fake_data_lr = all_fake_data[index:index + batch_size]\n",
    "    fake_data_hr = generator.predict(fake_data_lr)\n",
    "    all_fake_data_from_fake_lr.append(fake_data_hr)\n",
    "\n",
    "all_fake_data_from_fake_lr = np.concatenate(all_fake_data_from_fake_lr, 0)[:len(all_data)]\n",
    "assert all_fake_data_from_fake_lr.shape == all_data_hr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVNZN_AHJ2hl"
   },
   "outputs": [],
   "source": [
    "visualize_images(all_fake_data_from_fake_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I43NtR1Ayic-"
   },
   "outputs": [],
   "source": [
    "np.save(\"fake_data_from_fake_lr.npy\", all_fake_data_from_fake_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-oL1u1lrmZy"
   },
   "source": [
    "# III. GAN metrics (2 pts)\n",
    "\n",
    "There exists a few metrics used to measure GAN performance. Some of them are based on comparing real samples against generated ones, while the other rely on additional pretrained models that are applied to both real and generated data in order to accumulate high-level statistics. In this task, you are going to use two metrics representing these two approaches -- namely, [Precision-Recall Density](https://arxiv.org/pdf/1806.00035) and [Fréchet Inception Distance](https://arxiv.org/pdf/1706.08500). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyB2QrR2Yphl"
   },
   "source": [
    "### Precision-Recall Density (PRD score) (1 pt)\n",
    "\n",
    "Your first task is to implement [Precision-Recall Density](https://arxiv.org/pdf/1806.00035.pdf) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lROL3akPIH1v"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "def bin_counts(real_data, fake_data, n_bins=25):\n",
    "    real_data = real_data.reshape(len(real_data), -1)\n",
    "    fake_data = fake_data.reshape(len(fake_data), -1)\n",
    "\n",
    "    data = np.vstack([real_data, fake_data])\n",
    "\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_bins, n_init=10).fit(data)\n",
    "    \n",
    "    real_labels = kmeans.labels_[:len(real_data)]\n",
    "    fake_labels = kmeans.labels_[len(real_data):]\n",
    "\n",
    "    real_density = np.histogram(real_labels, bins=n_bins, range=[0, n_bins], density=True)[0]\n",
    "    fake_density = np.histogram(fake_labels, bins=n_bins, range=[0, n_bins], density=True)[0]\n",
    "    \n",
    "    return real_density, fake_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gE4RLHpOIH10"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def calculate_alpha_beta(real_density, fake_density, n_thetas=1000):\n",
    "    assert real_density.shape == fake_density.shape\n",
    "    \n",
    "    alpha = []\n",
    "    beta = []\n",
    "    \n",
    "    thetas = np.linspace(1e-6, np.pi / 2 - 1e-6, num=n_thetas)\n",
    "    for theta in thetas:\n",
    "        tan = math.tan(theta)\n",
    "        alpha.append(\"???\")\n",
    "        beta.append(\"???\")\n",
    "    \n",
    "    return alpha, beta\n",
    "\n",
    "def calculate_precision_recall_density(real_densities, fake_densities, repeat_number=1000):\n",
    "    vectors = np.array([\n",
    "       calculate_alpha_beta(real_densities, fake_densities)\n",
    "       for _ in range(repeat_number)\n",
    "    ]).mean(axis=0)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def calculate_prd_score(real_data, fake_data):\n",
    "    # Calculate bin counts from real and generated data\n",
    "    real_density, fake_density = \"???\"\n",
    "\n",
    "    plt.bar(range(len(real_density)), real_density, width=1, color='g', alpha=0.5, label=\"Real density\")\n",
    "    plt.bar(range(len(fake_density)), fake_density, width=1, color='r', alpha=0.5, label=\"Fake density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate alpha and beta\n",
    "    alpha, beta = \"???\"\n",
    "    \n",
    "    # Calculate area under curve (AUC) for alpha and beta\n",
    "    score = \"???\"\n",
    "\n",
    "    return score, alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8N59Rb9J2cv"
   },
   "source": [
    "Calculate PRD score for DCGAN (task I). You should pass `all_data` and `all_fake_data` to scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FeB4Xum6LJOY"
   },
   "outputs": [],
   "source": [
    "score,  _, _ = calculate_prd_score(all_data, all_fake_data)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SayfTgIpKIYV"
   },
   "source": [
    "Now use PRD score to compare high resolution data generated from real low resolution data (`all_fake_data_from_real_lr`) and fake resolution data (`all_fake_data_from_fake_lr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJptPFfRIH17"
   },
   "outputs": [],
   "source": [
    "print(\"Generated from real LR\")\n",
    "score_real, alpha_real, beta_real = calculate_prd_score(all_data_hr, all_fake_data_from_real_lr)\n",
    "print(\"Score:\", score_real, end='\\n\\n')\n",
    "\n",
    "print(\"Generated from fake LR\")\n",
    "score_fake, alpha_fake, beta_fake = calculate_prd_score(all_data_hr, all_fake_data_from_fake_lr)\n",
    "print(\"Score:\", score_fake, end='\\n\\n')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(alpha_real, beta_real, color='g', label=\"Generated from real LR\")\n",
    "plt.plot(alpha_fake, beta_fake, color='r', label=\"Generated from fake LR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLKjJZriZuxK"
   },
   "source": [
    "### Fréchet Inception Distance (FID score) (1 pt)\n",
    "\n",
    "[Frechet Inception Distance](https://arxiv.org/pdf/1706.08500) is an improved version of [Inception score](https://arxiv.org/abs/1606.03498), that additionally calculates the statistics of real data and compares it to the statistics of generated data. It is probably the most widely-used option for evaluating GANs, and relies on features extracted with [InceptionV3](https://arxiv.org/abs/1512.00567) pretrained on ImageNet. These features assumed to come from a multivariate Gaussian distribution, so Fréchet distance between two multivariate Gaussians can be calculated:\n",
    "\n",
    "$$\\text{FID} = ||\\mu_r - \\mu_g||^2 + \\text{Tr} (\\Sigma_r + \\Sigma_g - 2 (\\Sigma_r \\Sigma_g)^{1/2}),$$\n",
    "\n",
    "where $X_r \\sim \\mathcal{N} (\\mu_r, \\Sigma_r)$ and $X_g \\sim \\mathcal{N} (\\mu_g, \\Sigma_g)$ are the 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RsHvVgYMHFt"
   },
   "source": [
    "First, create InceptionV3 model. As you will be using it for feature extraction only, you should set `include_top=False` and `pooling=\"avg\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0M24Al7E7It"
   },
   "outputs": [],
   "source": [
    "base_inception = keras.applications.inception_v3.InceptionV3(weights=\"imagenet\",\n",
    "                                                             include_top=\"???\",\n",
    "                                                             pooling=\"???\")\n",
    "inputs = L.Input(shape=(112, 112, 3))\n",
    "outputs = base_inception(L.UpSampling2D(2)(inputs))\n",
    "\n",
    "inception = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "inception.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcriN6sVoweS"
   },
   "outputs": [],
   "source": [
    "real_data_lr, real_data_hr = sample_super_res_data(32)\n",
    "assert inception.predict(real_data_hr).shape[1:] == (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogSl9iYkqQ0z"
   },
   "outputs": [],
   "source": [
    "def calculate_activations(data, batch_size=32, verbose=False):\n",
    "    # Calculate activations of Pool3 layer of InceptionV3\n",
    "    if verbose:\n",
    "        print(\"Calculating activations...\")\n",
    "    activations = inception.predict(data, batch_size=batch_size, verbose=verbose)\n",
    "    return activations\n",
    "\n",
    "def calculate_activation_statistics(activations):\n",
    "    # Calculate mean and covariance of activations. Mind the dimensions!\n",
    "    mu = \"???\"\n",
    "    sigma = \"???\"\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgB7zO5Oc9-V"
   },
   "outputs": [],
   "source": [
    "real_activations = calculate_activations(all_data_hr, verbose=True)\n",
    "real_mu, real_sigma = calculate_activation_statistics(real_activations)\n",
    "\n",
    "assert real_mu.shape == (2048,)\n",
    "assert real_sigma.shape == (2048, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0epEymAvYmjy"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    assert mu1.shape == mu2.shape\n",
    "    assert sigma1.shape == sigma2.shape\n",
    "\n",
    "    sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1, sigma2))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(sigma1_sigma2):\n",
    "        sigma1_sigma2 = sigma1_sigma2.real\n",
    "\n",
    "    # Product might be almost singular\n",
    "    if not np.isfinite(sigma1_sigma2).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1 + offset, sigma2 + offset))\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    return \"???\" # use diff, sigma1, sigma2 to calculate FID according to the formula above\n",
    "\n",
    "\n",
    "def calculate_fid_score(real_data, fake_data, verbose=False):\n",
    "    # Run inception on real and fake data to obtain activations\n",
    "    real_activations = \"???\"\n",
    "    fake_activations = \"???\"\n",
    "\n",
    "    # Calculate mu and sigma for both real and fake activations\n",
    "    real_mu, real_sigma = \"???\"\n",
    "    fake_mu, fake_sigma = \"???\"\n",
    "\n",
    "    return \"???\" # calculate Frechet distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wU2PaqTRDWTv"
   },
   "source": [
    "Calculate FID score between `all_data_hr` and `all_fake_data_from_real_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXqqw2cgDq1X"
   },
   "outputs": [],
   "source": [
    "score = calculate_fid_score(all_data_hr, all_fake_data_from_real_lr)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_qyDJZTD_7r"
   },
   "source": [
    "Putting it all together: calculate FID score between `all_data_hr` and `all_fake_data_from_fake_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCjQoz-rDr2O"
   },
   "outputs": [],
   "source": [
    "score = calculate_fid_score(all_data_hr, all_fake_data_from_fake_lr)\n",
    "print(\"Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
